\subsection{Concrete Version of the Poincar\'{e}--Birkhoff--Witt Theorem}


\begin{convention}
  For this section we fix a Lie algebra~$\glie$ with basis~$(x_i)_{i \in I}$.
  For better readability we will denote for every~$x \in \glie$ the corresponding element~$\class{x} \in \Univ(\glie)$ simply by~$x$ again.
\end{convention}


\begin{remark}
  As a consequence of the upcoming PBW~theorem we will see in \cref{embedding into uea} that for any Lie~algebra~$\glie$ the canonical homomorphism~$\glie \to \Univ(\glie)$ is actually injective, allowing us to identify~$\glie$ with a Lie~subalgebra~$\Univ(\glie)$.
  This will a posteriori justify our abuse of notation.
\end{remark}


\begin{fluff}
  Let~$A \defined \kf\gen{X,Y}/(YX - XY - 1)$ be the Weyl~algebra with the filtration induced from the standard grading of~$\kf\gen{X,Y}$ and let~$x, y \in A$ denote the residue classes of~$X$ and~$Y$.
  We have seen in \cref{weyl algebra} that~$A$ has a basis given by all monomials~$x^n y^m$ with~$n, m \geq 0$ and that the associated graded algebra~$\gr(A)$ does therefore have a basis given by all residue classes~$[x^n y^m]_i$ with~$n, m \geq 0$ and~$i = n+m$.
  Moreover, the multiplication of~$\gr(A)$ is on these basis elements given by~$[x^n y^m]_i \cdot [x^k y^l]_j = [x^{n+k} y^{m+l}]_{i+j}$ for all~$n, m, k, l \geq 0$ and~$i = n+m$,~$j = k+l$. 
  
  The key observation behind these results was the relation~$yx = xy + 1$ in~$A$, which tells us that the elements~$x$ and~$y$ commute up to terms of smaller degre, and hence actually commute in~$\gr(A)$ (in the sense that the associated elements~$[x]_1, [y]_1 \in \gr(A)$ commute).
  For the universal enveloping algebra~$\Univ(\glie)$ a similar situation occurs:
  For any two elements~$x, y \in \glie$ the relation~$yx = xy + [x,y]$ holds in~$\Univ(\glie)$.
  Hence the elements~$x$ and~$y$ ought to commute up to a term of smaller degree in~$\Univ(\glie)$.
  We will therefore try to generalize the above results from~$A$ to~$\Univ(\glie)$ to describe a vector space basis of~$\Univ(\glie)$.
  
  We already know that~$\Univ(\glie)$ is generated by the image of~$\glie$ under the canonical map~$\glie \to \Univ(\glie)$.
  Hence~$\Univ(\glie)$ is spanned by the elements~$x_i$ with~$i \in I$.
  This means that the momomials~$x_{i_1} \dotsm x_{i_n}$ with~$n \geq 0$ and~$i_1, \dotsc, i_n \in I$ span~$\Univ(\glie)$ as a vector space.
  By the above discussion we should hopefully be able to rearrange the terms~$x_{i_j}$ in these monomials without destroying the property of being a vector space generating set.
  We will see in \cref{pbw concrete generating part} that this is indeed the case.
  
  We will overall show in \cref{pbw concrete} that if we fix an ordering of the basis elements~$x_i$ then the collection of all ordered monomials will form a basis of the universal enveloping algebra~$\Univ(\glie)$.
  We will also see un \cref{pbw abstract} that this is equivalent to~$\gr(\Univ(\glie))$ being isomorphic (as graded algebras) to the symmetric algebra~$\Symm(\glie)$ and hence a polynomial algebra~$\kf[t_i \suchthat i \in I]$.
\end{fluff}


\begin{lemma}
  \label{rearranging lemma}
  There exists for all~$y_1, \dotsc, y_n \in \glie$ and every permutation~$\sigma \in S_n$ some error term~$r \in \Univ(\glie)_{(n-1)}$ with
  \[
    y_1 \dotsm y_n
    =
    y_{\sigma(1)} \dotsm y_{\sigma(n)}
    +
    r  \,.
  \]
\end{lemma}


\begin{proof}
  It sufficies to prove the statement for~$\sigma$ being a simple transposition because these generate the symmetric group~$S_n$.
  So let~$\sigma = (i, i+1)$.
  Then
  \[
    y_i y_{i+1}
    =
    y_{i+1} y_i + [y_i, y_{i+1}]
  \]
  and hence
  \begin{align*}
    y_1 \dotsm y_n
    =
    y_1 \dotsm y_i y_{i+1} \dotsm y_n
    =
    (y_1 \dotsm y_{i+1} y_i \dotsm y_n)
    +
    (y_1 \dotsm [y_i, y_{i+1}] \dotsm y_n)
  \end{align*}
  with~$y_1 \dotsm [y_i, y_{i+1}] \dotsm y_n \in \Univ(\glie)_{(n-1)}$.
\end{proof}


\begin{convention}
  We suppose that~$(I, \leq)$ is a totally ordered set.
  For every~$n \geq 0$ we set
  \[
    I^n
    \defined
    \{
      (i_1, \dotsc, i_n)
    \suchthat
      \text{$i_1, \dotsc, i_n \in I$ with~$i_1 \leq \dotsb \leq i_n$}
    \}
  \]
  as well as~$I^{(n)} \defined \bigcup_{m=0}^n I^m$ and overall~$I^* \defined \bigcup_{n \geq 0} I^n$.
  For every tupel~$\alpha \in I^*$ with~$\alpha = (i_1, \dotsc, i_n)$ we define the associated \defemph{ordered monomial}~$\gls*{ordered monomial} \in \Univ(\glie)$\index{ordered monomial} as
  \[
    x_\alpha
    \defined
    x_{i_1} \dotsm x_{i_n}  \,.
  \]
  
  For~$\alpha, \beta \in I^*$ we denote by~$\alpha \cdot \beta \in I^*$ the tupel that results from the concatenation of~$\alpha$ and~$\beta$ by reordering of the entries.
  More explicitely, if~$\alpha = (i_1, \dotsc, i_n) \in I^n$ and~$\beta = (j_1, \dotsc, j_m) \in I^m$ then by reordering the entries of the concatenation~$(i_1, \dotsc, i_n, j_1, \dotsc, j_m)$ into increasing order we arrive at~$\alpha \cdot \beta \in I^{n+m}$.
  For~$i \in I$ and~$\alpha \in I^*$ we define~$i \cdot \alpha \in I^*$ by identifying~$I$ with~$I^1$.
  More explicitely, if~$\alpha = (i_1, \dotsc, i_n)$ with~$i_1 \leq \dotsb \leq i_k \leq i \leq i_{k+1} \leq \dotsb \leq i_n$ then~$i \cdot \alpha = (i_1, \dotsc, i_k, i, i_{k+1}, \dotsc, i_n)$.
  
  If~$i \in I$ and~$\alpha \in I^*$ with~$\alpha = (i_1, \dotsc, i_n)$ then we write~$i \leq \alpha$ to mean that~$i \leq i_1$.
  For~$\alpha = ()$ we have~$i \leq \alpha$ for every~$i \in I$ by definition.
\end{convention}


\begin{theorem}[Poincar\'{e}--Birkhoff--Witt (concrete version)]
  \label{pbw concrete}
  The elements~$x_\alpha$ with~$\alpha \in I^*$ form a~{\basis{$\kf$}} of~$\Univ(\glie)$.%
  \footnote{This includes that they are pairwise different.}
\end{theorem}


\begin{remark}
  The proposed basis~$x_\alpha$ with~$\alpha \in I^*$ may be written as
  \[
    x_{i_1}^{p_1} \dotsm x_{i_n}^{p_n}
    \qquad
    \text{with~$n \geq 0$,~$i_1, \dotsc, i_n \in I$,~$i_1 < \dotsb < i_n$,~$p_1, \dotsc, p_n \geq 1$} \,,
  \]
  which explains why we call them \emph{ordered monomials}.
\end{remark}


\begin{example}
  \leavevmode
  \begin{enumerate}
    \item
      If~$\glie$ is a finite dimensional Lie algebra with basis~$x_1, \dotsc, x_n$ then the algebra~$\Univ(\glie)$ has the ordered monomials~$x_1^{p_1} \dotsm x_n^{p_n}$ with~$p_1, \dotsc, p_n \geq 0$ as a basis.
    \item
      A basis of~$\Univ(\sllie_2(\kf))$ is given by the ordered monomials~$e^n h^m f^k$ with~$n, m, k \geq 0$.
  \end{enumerate}
\end{example}


\begin{example}
  Once the PBW~theorem is proven we can give a counterexample for \cref{generators of associated graded}:
  Let~$\glie = \sllie_2(\kf)$ and let~$A \defined \Univ(\glie)$ with the standard filtration induced from the standard grading of the tensor algebra~$\Tensor(\glie)$.
  We already know that~$\Univ(\glie)$ is generated as a~{\algebra{$\kf$}} by the elements~$e$,~$h$~and~$f$.
  It follows from the relation~$[e,f] = h$ that~$A$ is already generated by the two elements~$e$~and~$f$.
  But the associated graded algebra~$\gr(A)$ is not generated by the associated elements~$[e]_1$ and~$[f]_1$ because they only result is the basis elements~$e^n f^k$ with~$n, k \geq 0$.
  We are hence missing all basis elements~$e^n h^m f^k$ with~$n, k \geq 0$ and~$m \geq 1$.
\end{example}


\begin{lemma}
  \label{pbw concrete generating part filtered part}
  The ordered monomials~$x_\alpha$ with~$\alpha \in I^{(n)}$ span the vector space~$\Univ(\glie)_{(n)}$.
\end{lemma}


\begin{proof}
  We show the \lcnamecref{pbw concrete generating part filtered part} by induction over~$n$.
  For~$n = 0$ we have
  \[
    \Univ(\glie)_{(0)}
    =
    \kf
    =
    \gen{ 1 }_{\kf}
    =
    \gen{ x_{()} }_{\kf}
    =
    \gen{ x_\alpha \suchthat \alpha \in I^{(0)} }_{\kf}
  \]
  where~$()$ denotes the empty tupel, the only element of~$I^{(0)} = I^{0}$.
  
  Suppose now that the statement holds for some~$n \geq 0$.
  The homogeneous part~$\gr(\Univ(\glie))_{(n+1)}$ is spanned by all monomials~$x_{i_1} \dotsm x_{i_{n+1}}$ with~$i_1, \dotsc, i_{n+1} \in I$.
  It hence sufficies to show that every such monomials can be expressed via the proposed generators~$x_\alpha$ with~$\alpha \in I^{(n+1)}$.
  We know from \cref{rearranging lemma} that
  \[
    x_{i_1} \dotsm x_{i_{n+1}}
    =
    x_\alpha + r
  \]
  for some~$\alpha \in I^{(n+1)}$ and~$r \in \gr(\Univ(\glie))_{(n)}$ by rearranging the factors~$x_{i_j}$.
  The additional term~$r$ can by induction hypothesis be expressed as a linear combination of those~$x_\beta$ with~$\beta \in I^{(n)}$.
  The claim now follows because~$I^{(n)} \subseteq I^{(n+1)}$.
\end{proof}


\begin{corollary}
  \label{pbw concrete generating part}
  The elements~$x_\alpha$ with~$\alpha \in I^*$ span the algebra~$\Univ(\glie)$ as a vector space.
  \qed
\end{corollary}


\begin{construction}
  To show the linear independence of the elements~$x_\alpha$ with~$\alpha \in I^*$ we use a general construction from representation theory that we’re about to introduce:
  
  Suppose that we are given a vector space generating set~$(b_j)_{j \in J}$ of~$A$, which we would like to show to be a basis of~$A$.
  For this we construct an~{\module{$A$}}~$M$ together with an element~$m_0 \in M$ such that the elements~$b_j m_0$ with~$j \in J$ are linearly dependent in~$M$.
  It then follows that the elements~$b_j$ with$~j \in J$ are linearly independent.
  
  The main idea behind the construction of~$M$ --- and why such a module should exist in the first place --- is that if the elements~$b_j$ with~$j \in J$ are indeed a basis for$~A$ then we could choose~$M = A$ and~$m_0 = 1$.
  To construct the module~$M$ we therefore start by taking as its underlying vector space the free vector space on a basis~$B_j$ with~$j \in J$.
  We then want~$A$ to act on~$M$ in the same way as it acts on itself.
  
  How exactly the construction of this~{\module{$A$}} structure on~$M$ can be achieved depends on the given situation.
  We will outline two strategies here:
  The first is a very general one, whereas the second one will be used for the proof of the PBW~theorem:
  \begin{enumerate}
    \item
      We assume that~$A$ is given by some algebra generators~$x_i$ with~$i \in I$ and some relations between these generators.
      We can encode the action of the generators~$x_i$ on~$A$ by considering for all~$i \in I$ and~$j \in J$ a linear combination~$x_i b_j = \sum_{k \in J} c_{ij}^k b_k$ (if the~$b_j$ are indeed a basis for~$A$ then the coefficients~$c_{ij}^k$ will a posteriori turn out to be unique).
      We want to define an action of~$A$ on~$M$ via~$x_i B_j = \sum_{k \in J} c_{ij}^k B_k$ for all~$i \in I$ and~$j \in J$.
      We do so in two steps:
      \begin{enumerate}
        \item
          We first define a~{\module{$\kf\gen{X_i \suchthat i \in I}$}} structure on~$M$ via~$X_i \cdot B_j = \sum_{k \in I} c_{ij}^k B_k$.
          We can do this because a~{\module{$\kf\gen{X_i \suchthat i \in I}$}} structure on~$M$ is the same an algebra homomorphism~$\kf\gen{X_i \suchthat i \in I} \to \End_{\kf}(M)$, which can be constructed by using the universal property of~$\kf\gen{X_i \suchthat i \in I}$.
        \item
          We then check that this action descends to an~{\module{$A$}} structure on~$M$ by checking that it is compatible with the given relations between the algebra generators~$x_i$.
      \end{enumerate}
    \item
      Suppose that~$A = \Univ(\glie)$ is the universal enveloping algebra of some Lie~algebra~$\glie$.
      Then an~{\module{$A$}} structure on~$M$ is the same as letting~$\glie$ act on~$M$ so that~$x.y.m - y.x.m = [x,y].m$ for all~$x, y \in \glie$ and~$m \in M$.
      We will therefore try to construct such an action instead.
      We will do so in the given situation by considering a filtration~$M = \bigcup_{n \geq 0} M_{(0)}$ by linear subspaces~$M_{(i)}$ of~$M$ and then inductively construct partial actions~$\varphi_n \colon \glie \times M_{(n)} \to M_{(n+1)}$ that extend each other.
  \end{enumerate}
  
  For the last step we assume that~$1_A = b_k$ for some~$k \in J$.
  We then show that for~$m_0 \defined B_k$ we have~$b_j m_0 = B_j$ for all~$j \in J$, giving us the desired linear independence of the elements~$b_j m_0$ with~$j \in J$.

  We observe that if~$(b_j)_{j \in J}$ is indeed a basis of~$A$ then this construction of the~{\module{$A$}}~$M$ has to go through, as it is just an artifical reconstruction of the welldefined~{\module{$A$}}~$M = A$.
  So if the construction succeeds then we have shown that~$(b_j)_{j \in J}$ is indeed a basis for~$A$, but if the cnostruction fails then we have shown that it’s not a basis.
\end{construction}


\begin{example}
  \label{linear independence for weyl algebra}
  Let us return to the Weyl~algebra~$A \defined \kf\gen{X, Y}/(YX - XY - 1)$\index{Weyl!algebra} from \cref{weyl algebra}.
  We again denote the residue classes of~$X$ and~$Y$ in~$A$ by~$x$ and~$y$.
  We still need to prove \cref{linear independence of monomials} from \cref{weyl algebra}, that the monomials~$x^n y^m$ with~$n, m \geq 0$ are linearly independent in~$A$.
  
  Let~$M$ be the free vector space on a basis~$T^n U^m$ with~$n, m \geq 0$.
  (The basis element~$T^n U^m$ of~$M$ corresponds to the proposed basis element~$x^n y^m$ of~$A$.)
  The action of the two algebra generators~$x$ and~$y$ of~$A$ on the vector space generators~$x^n y^m$ of~$A$ is given by
  \begin{align*}
    x \cdot x^n y^m
    &=
    x^{n+1} y^m \,,
    \\
    y \cdot x^n y^m
    &=
    x^n y^{m+1} + n x^{n-1} y^m
  \end{align*}
  for all~$n, m \geq 0$.
  We therefore define a~{\module{$\kf\gen{X,Y}$}} structure on~$M$ by
  \begin{align*}
    X \cdot T^n U^m
    &=
    T^{n+1} U^m \,,
    \\
    Y \cdot T^n U^m
    &=
    T^n U^{m+1} + n T^{n-1} U^m
  \end{align*}
  for all~$m, n \geq 0$.
  This action descends to an~{\module{$A$}} structure on~$M$ because for all~$m, n \geq 0$,
  \begin{gather*}
    YX \cdot T^n U^m
    =
    Y \cdot T^{n+1} U^m
    =
    T^{n+1} U^{m+1} + (n+1) T^n U^m
  \shortintertext{and}
    XY \cdot T^n U^m
    =
    X \cdot ( T^n U^{m+1} + n T^{n-1} U^m )
    =
    T^{n+1} U^{m+1} + n T^n U^m
  \end{gather*}
  and hence
  \[
    (YX - XY - 1) \cdot T^n U^m
    =
    0 \,.
  \]
  We have that for every~$m \geq 0$ that
  \[
    y \cdot T^0 U^m
    =
    Y \cdot T^0 U^m
    =
    T^0 U^{m+1}
  \]
  and hence inductively
  \[
    y^k \cdot T^0 U^m
    =
    T^0 U^{m+k}
  \]
  for every~$k \geq 0$.
  It follows that
  \[
    x^n y^m \cdot T^0 U^0
    =
    x^n \cdot T^0 U^m
    =
    T^n U^m
  \]
  for all~$n, m \geq 0$.
  It now follows from the linear independence of the basis elements~$T^n U^m$ of~$M$ that the elements~$x^n y^m$ are linearly independent in~$A$.
\end{example}


\begin{example}
  Similar approaches can be applied when it comes to other algebraic structures, as for example groups:
  \begin{enumerate}
    \item
      If~$X$ is a set and~$F$ is the free group on~$X$ then every element of~$F$ can be uniquely written as a reduced word in~$X^{\pm 1}$.
      The uniqueness of such a reduced expression can be shown by letting~$F$ act on the set of all reduced words.
    \item
      Let~$G_i$ with~$i \in I$ be a collection of groups and let~$H$ be a group.
      For every~$i \in I$ let~$f_i \colon H \to G_i$ be a group homomorphism.
      Then elements of the resulting amalgamated product~$A = \ast_{H,i \in I} G_i$ can be represented in certain a canonical form, the uniqueness of which can be shown by letting~$A$ act on the set of all possible canonical forms.
      We refer to \cite[\S 1.2]{trees} for more information on this example.
  \end{enumerate}
\end{example}


\begin{proof}[Proof of \cref{pbw concrete}]
  By \cref{pbw concrete generating part} the elements~$x_\alpha$ with~$\alpha \in I^*$ span~$\Univ(\glie)$ as a vector space, but we still need to show that they are linearly independent.
  
  Let~$V$ be a vector space with basis~$X_{i_1} \dotsm X_{i_n}$ where~$n \geq 0$ and~$(i_1, \dotsc, i_n) \in I^n$.
  This means that for every~$\alpha \in I^*$ with~$\alpha = (i_1, \dotsc, i_n)$ we have an associated basis element
  \[
    X_\alpha
    \defined
    X_{i_1} \dotsm X_{i_n}
  \]
  of~$V$.
  For every~$n \geq 0$ we denote by~$V_{(n)} \subseteq V$ the~{\linear{$\kf$}} subspace spanned by all~$X_\alpha$ with~$\alpha \in I^{(n)}$.

  \begin{claim*}
    There exists a unique sequence~$(\varphi_n)_{n \geq 0}$ of bilinear maps
    \[
      \varphi_n
      \colon
      \glie \times V_{(n)}
      \to
      V_{(n+1)},
      \quad
      (x,p)
      \mapsto
      x.p
    \]
    that satisfy the following conditions:
    \begin{enumerate}
    \item
      \label{pbw restriction coincides}
      The restriction of~$\varphi_{n+1}$ to~$\glie \times V_{(n)}$ coincides with~$\varphi_n$ for every~$n \geq 0$.%
      \footnote{This condition actually follows from the other conditions by the uniqueness of the sequence~$(\varphi_n)_{n \geq 0}$.
    See \cite[\S 17.4]{Humphreys} for more details on this.}
    \item
      \label{pbw representation of lie algebra}
      $x_i.x_j.X_\alpha - x_j.x_i.X_\alpha = [x_i, x_j].X_\alpha$ for all~$i,j \in I$ and~$\alpha \in I^*$.
    \item
      \label{pbw essential condition}
      $x_i.X_\alpha = X_{i \cdot \alpha}$ for all~$i \in I$ and~$\alpha \in I^*$ with $i \leq \alpha$.
    \item
      \label{pbw technical detail for construction}
      $x_i.X_\alpha \equiv X_{i \cdot \alpha} \pmod{V_{(n)}}$ for all~$n \geq 0$,~$i \in I$ and~$\alpha \in I^{(n)}$.
    \end{enumerate}
 \end{claim*}

  \begin{proof}
    Let us first point out that the notation~$x.p$ with~$x \in \glie$ and~$p \in V$ is unambiguous by condition~\ref{pbw restriction coincides};
    we do not need to worry about which~$\varphi_n$ it refers to.
    The maps~$\varphi_n$ will be defined by induction over~$n \geq 0$:
    
    To define the map~$\varphi_0$ we do not have to consider condition~\ref*{pbw restriction coincides}.
    The linear subspace~$V_{(0)}$ is {\onedimensional} and spanned by the single element $X_{()}$.
    We see from condition~\ref*{pbw essential condition} that we need
    \[
      x_i.1
      =
      x_i.X_{()}
      =
      X_{i \cdot ()}
      =
      X_i
    \]
    for every~$i \in I$.
    We take this as the definition for~$\varphi_0$.
    Conditions~\ref*{pbw essential condition} hence hold by choice of~$\varphi_0$~and condition~\ref*{pbw technical detail for construction} follows from this (because always~$i \leq \alpha$ for~$n = 0$).
    Condition~\ref*{pbw representation of lie algebra} does not affect the case~$n = 0$ yet.
  
    Let now~$n \geq 0$ and suppose that~$\varphi_m$ is constructed for every~$m \leq n$.
    To construct~$\varphi_{n+1}$ we need by condition~\ref*{pbw restriction coincides} define only~$x_i.X_\alpha$ for~$\alpha \in I^{n+1}$.
    
    If~$i \leq \alpha$ then we set
    \begin{equation}
      \label{formula for smaller case}
      x_i.X_\alpha
      \defined
      X_{i \cdot \alpha}
    \end{equation}
    to ensure condition~\ref*{pbw essential condition} for~$\varphi_{n+1}$.
    Otherwise we can write~$\alpha = j \cdot \beta$ for some~$j \in I$ and~$\beta \in I^n$ with~$j \leq \beta$ and~$i > j$.
    It then follows from condition~\ref*{pbw representation of lie algebra} and~\ref*{pbw essential condition} that we need
    \[
      x_i.X_\alpha
      =
      x_i.X_{j \cdot \beta}
      =
      x_i.x_j.X_\beta
      =
      x_j.x_i.X_\beta + [x_i, x_j].X_\beta  \,.
    \]
    The second summand~$[x_i, x_j].X_\beta$ is already defined by induction hypothesis.
    For the first summand we observe that by condition~\ref*{pbw technical detail for construction} for~$\varphi_n$ we have
    \[
      x_i.X_\beta
      \equiv
      X_{i \cdot \beta}
      \mod
      V_{(n)}
    \]
    and hence~$x_i.X_\beta = X_{i \cdot \beta} + R$ for some unique error term~$R \in V_{(n)}$.
    It follows from~$j < i$ and~$j \leq \beta$ and that~$j \leq i \cdot \beta$.
    Hence the product~$x_j.X_{i \cdot \beta}$ needs to be given by
    \[
      x_j.X_{i \cdot \beta}
      =
      X_{j \cdot i \cdot \beta}
      =
      X_{i \cdot j \cdot \beta}
      =
      X_{i \cdot \alpha}  \,.
    \]
    Altogether we need that
    \begin{equation}
      \label{formula for bigger case}
      \begin{aligned}
        x_i.X_\alpha
        &=
        x_i.X_{j \cdot \beta}
        \\
        &=
        x_i.x_j.X_\beta
        \\
        &=
        x_j.x_i.X_\beta + [x_i, x_j].X_\beta
        \\
        &=
        x_j.(X_{i \cdot \beta} + R) + [x_i, x_j].X_\beta
        \\
        &=
        x_j.X_{i \cdot \beta} + x_j.R + [x_i, x_j].X_\beta
        \\
        &=
        X_{i \cdot \alpha} + x_j.R + [x_i, x_j].X_\beta
      \end{aligned}
    \end{equation}
    with~$j$ the first term of~$\alpha$,~$\beta$ the rest of~$\alpha$ and~$R = x_i.X_\beta - X_{i \cdot \beta}$.
    Every term of the last expression is already defined by~$\varphi_n$, which shows the uniqueness of~$\varphi_{n+1}$.
    We take~\eqref{formula for bigger case} as the definition of~$x_i.X_\alpha$ for~$i \nleq \alpha$.
    
    We overall define~$\varphi_{n+1}$ as the unique bilinear extension of~$\varphi_n$ such that~$x_i.X_\alpha$ is given on the additional elements with~$\alpha \in I^{n+1}$ given by~\eqref{formula for smaller case} and~\eqref{formula for bigger case} (depending on whether~$i \leq \alpha$ or not).
    
    Condition~\ref*{pbw restriction coincides} holds by construction of~$\varphi_{n+1}$ an an extension of~$\varphi_n$.
    Condition~\ref*{pbw essential condition} also holds by construction, namely by the corresponding property of~$\varphi_n$ together with the definition of~$x_i.X_\alpha$ for~$i \leq \alpha$ and~$\alpha \in I^{n+1}$ by formula~\eqref{formula for smaller case}.
    Condition~\ref*{pbw technical detail for construction} holds in the case~$i \leq \alpha$ by condition~\ref*{pbw essential condition} and otherwise by the corresponding property for~$\varphi_n$ or for~$\alpha \in I^{n+1}$ by the definition of~$\varphi_{n+1}$ via~\eqref{formula for bigger case}, because
    \[
      x_i.X_\alpha - X_{i.\alpha}
      =
      x_j.R + [x_i, x_j].X_\beta
      \in
      V_{(n)} \,.
    \]
  
    It remains to check condition~\ref*{pbw representation of lie algebra} for~$\varphi_{n+1}$.
    We only need to check this condition for~$i,j \in I$ and~$\alpha \in I^n$ as all other cases are covered by the corresponding property of~$\varphi_n$.
    For~$i = j$ the needed equality follows from the Lie~bracket being alternating.
    We therefore consider in the following only the case~$i \neq j$.
    We consider different cases:
    
    \begin{description}
      \item[Case~1 \textup($i \leq \alpha$\textup):]
        We consider two subcases:
        \begin{description}
          \item[Case~1.1 \textup($i \leq \alpha$,~$i < j$\textup):]
            In this case~$j \nleq i \cdot \alpha$ and we can apply (the first lines of)~\eqref{formula for bigger case} to find that
            \[
              x_j.x_i.X_\alpha
              =
              x_j.X_{i \cdot \alpha}
              =
              x_i.x_j.X_\alpha + [x_j,x_i].X_\alpha \,.
            \]
            By rearranging this equation we get the desired equality for the given case.
          \item[Case~1.2 \textup($i \leq \alpha$,~$i > j$\textup):]
            It follows in this case that~$j \leq \alpha$.
            We therefore find by the previous case that
            \[
              x_j.x_i.X_\alpha - x_i.x_j.X_\alpha
              =
              [x_j, x_i].X_\alpha \,.
            \]
            By using that the Lie~bracket~$[-,-]$ is skew symmetric we get the desired equality
            \[
              [x_i, x_j].X_\alpha
              =
              -[x_j, x_i].X_\alpha
              =
              -(x_j.x_i.X_\alpha - x_i.x_j.X_\alpha)
              =
              x_i.x_j.X_\alpha - x_j.x_i.X_\alpha \,.
            \]
        \end{description}
      \item[Case~2 \textup($j \leq \alpha$\textup):]
        This follows from the previous case because the Lie~bracket~$[-,-]$ is skew-symmetric.
      \item[Case~3 \textup($i \nleq \alpha$,~$j \nleq \alpha$\textup):]
        This case cannot happen for~$n = 0$ because then~$i \leq () = \alpha$.
        So we may assume that~$n \geq 1$.
        We can then split up the index~$\alpha$ as~$\alpha = k \cdot \beta$ where~$k \in I$ is the first term of~$\alpha$ and~$\beta \in I^{n-1}$ is the rest of~$\alpha$.
        We then have
        \[
          X_\alpha
          =
          X_{k \cdot \beta}
          =
          x_k.X_\beta \,.
        \]
        We know from the induction hypothesis that condition~\ref*{pbw representation of lie algebra} holds for~$\varphi_n$.
        We therefore have
        \[
          x_j.X_\alpha
          =
          x_j.x_k.X_\beta
          =
          x_k.x_j.X_\beta + [x_j, x_k].X_\beta  \,.
        \]
        By acting with~$x_i$ on this equation we find the equality
        \begin{equation}
          \label{long equation}
          x_i.x_j.X_\alpha
          =
          x_i.x_k.x_j.X_\beta + x_i.[x_j,x_k].X_\beta \,.
        \end{equation}
        To reshape the first summand we want to apply condition~\ref*{pbw representation of lie algebra} to find for the element~$Y = x_j.X_\beta$ that
        \begin{equation}
          \label{wanted equation for pbw}
          x_i.x_k.Y
          =
            x_k.x_i.Y
          + [x_i, x_k].Y  \,.
        \end{equation}
        But for this we need to explain why condition~\ref*{pbw representation of lie algebra} can be applied.
        We observe that
        \[
          x_j.X_\beta
          \equiv
          X_{j \cdot \beta}
          \mod
          V_{(n-1)}
        \]
        by condition~\ref*{pbw technical detail for construction}, so that
        \[
          Y
          =
          x_j.X_\beta
          =
          X_{j \cdot \beta} + R 
        \]
        for some rest term~$R \in V_{(n-1)}$.
        The desired equation~\eqref{wanted equation for pbw} holds when we replace~$Y$ by~$R$ because~$\varphi_n$ satisfies condition~\ref*{pbw representation of lie algebra} for every~$X_\gamma$ with~$\gamma \in I^{(n-1)}$ and therefore for every~$R' \in V_{(n-1)}$.
        It follows from the previous cases that equation~\eqref{wanted equation for pbw} also holds when we replace~$Y$ by~$X_{j \cdot \beta}$ because it follows from~$k < j$ and~$k \leq \beta$ that~$k \leq j \cdot \beta$.5
        It follows by additivity that equation~\eqref{wanted equation for pbw} also holds for~$Y = X_{j \cdot \beta} + R$.
        
        By inserting~\eqref{wanted equation for pbw} with~$Y = x_j.X_\beta$ into the identity~\eqref{long equation} we find
        \[
          x_i.x_j.X_\alpha
          =
            x_k.x_i.x_j.X_\beta
          + [x_i, x_k].x_j.X_\beta
          + x_i.[x_j,x_k].X_\beta \,.
        \]
        We may swap the roles of~$i$ and~$j$ to also find
        \[
          x_j.x_i.X_\alpha
          =
            x_k.x_j.x_i.X_\beta
          + [x_j, x_k].x_i.X_\beta
          + x_j.[x_i,x_k].X_\beta \,.
        \]
        By subtracting these two identities we find
        \begin{align*}
          {}&
          x_i.x_j.X_\alpha - x_j.x_i.X_\alpha
          \\
          ={}&
            x_k.x_i.x_j.X_\beta
          + [x_i, x_k].x_j.X_\beta
          + x_i.[x_j,x_k].X_\beta
          - x_k.x_j.x_i.X_\beta
          - [x_j, x_k].x_i.X_\beta
          - x_j.[x_i,x_k].X_\beta \,.
        \end{align*}
        By using that condition~\ref*{pbw representation of lie algebra} holds for~$\varphi_n$ we find that
        \begin{align*}
          {}&
            \underbrace{ x_k.x_i.x_j.X_\beta }_{1}
          + \underbrace{ [x_i, x_k].x_j.X_\beta }_{2}
          + \underbrace{ x_i.[x_j,x_k].X_\beta }_{3}
          - \underbrace{ x_k.x_j.x_i.X_\beta }_{1}
          - \underbrace{ [x_j, x_k].x_i.X_\beta }_{3}
          - \underbrace{ x_j.[x_i,x_k].X_\beta }_{2}
          \\
          ={}&
            x_k.[x_i, x_j].X_\beta
          + [[x_i, x_k], x_j].X_\beta
          + [x_i, [x_j, x_k]].X_\beta \,.
        \end{align*}
        It follows from the Jacobi~identity that
        \[
          [[x_i, x_k], x_j] + [x_i, [x_j, x_k]]
          =
          [x_j, [x_k, x_i]] + [x_i, [x_j, x_k]]
          =
          -[x_k, [x_i, x_j]]
        \]
        and therefore
        \begin{align*}
          {}&
            x_k.[x_i, x_j].X_\beta
          + [[x_i, x_k], x_j].X_\beta
          + [x_i, [x_j, x_k]].X_\beta
          \\
          ={}&
            x_k.[x_i, x_j].X_\beta
          - [x_k, [x_i, x_j]].X_\beta
          \\
          ={}&
            x_k.[x_i, x_j].X_\beta
          - x_k.[x_i, x_j].X_\beta
          + [x_i, x_j].x_k.X_\beta
          \\
          ={}&
          [x_i, x_j].x_k.X_\beta
          \\
          ={}&
          [x_i, x_j].X_{k \cdot \beta}
          \\
          ={}&
          [x_i, x_j].X_\alpha \,.
        \end{align*}
        We used for the last two steps that~$k \leq \beta$ with~$k \cdot \beta = \alpha$ by choice of~$\alpha$ and~$\beta$.
        This shows altogether that
        \[
          x_i.x_j.X_\alpha - x_j.x_i.X_\alpha
          =
          [x_i, x_j].X_\alpha
        \]
        as desired.
      \qedhere
    \end{description}
  \end{proof}
  
  Condition~\ref*{pbw restriction coincides} ensures that the maps~$(\varphi_n)_{n \geq 0}$ combine into a single bilinear map~$\varphi \colon \glie \times V \to V$.
  Condition~\ref*{pbw representation of lie algebra} means that~$\varphi$ makes~$V$ into a~{\representation{$\glie$}}.
  The action~$\varphi$ corresponds to an~{\module{$\Univ(\glie)$}} structure on~$V$.
  For every~$\alpha \in I^*$ with~$\alpha = (i_1, \dotsc, i_n)$ we have~$i_1 \leq \dotsb \leq i_n$ and hence
  \[
    x_\alpha . X_{()}
    =
    x_{i_1} \dotsm x_{i_n} \cdot X_{()}
    =
    x_{i_1} \dotsm x_{i_{n-1}} \cdot X_{(i_n)}
    =
    \dotsb
    =
    X_{(i_1, \dotsc, i_n)}
    =
    X_\alpha  \,.
  \]
  It follows from the linear independence of the elements~$X_\alpha$ with~$\alpha \in I^*$ in~$V$ that the elements~$x_\alpha$ with~$\alpha \in I^*$ are linearly independent in~$\glie$.
\end{proof}


\begin{corollary}
  \label{pbw concrete basis part filtered part}
  For every~$n \geq 0$ the filtered part~$\Univ(\glie)_{(n)}$ has the monomials~$x_\alpha$ with~$\alpha \in I^{(n)}$ as a basis.
\end{corollary}


\begin{proof}
  These monomials are a vector space generating set by \cref{pbw concrete generating part filtered part} and they are linearly independent by the PBW~theorem.
\end{proof}


\begin{corollary}
  \label{embedding into uea}
  For any Lie~algebra~$\glie$ the canonical homomorphism~$\glie \to \Univ(\glie)$ is injective.
\end{corollary}


\begin{proof}
  If~$(x_i)_{i \in I}$ is any basis of~$\glie$ thes the associated residue classes~$\class{x_i} \in \Univ(\glie)$ are part of a basis of~$\Univ(\glie)$ by the PBW~theorem and hence linearly independent.
\end{proof}


\begin{remark}
  By \cref{embedding into uea} we may identify any Lie~algebra~$\glie$ with its image in its universal enveloping algebra~$\Univ(\glie)$.
  In the rest of these notes we will therefore regard every Lie~algebra~$\glie$ as a Lie~subalgebra of its universal enveloping algebra and the canonical homomorphism~$\glie \to \Univ(\glie)$ as the inclusion.
\end{remark}


\begin{corollary}[Existence of free Lie~algebras]
  For every set~$I$ the free~{\liealgebra{$\kf$}}\index{free Lie algebra}\index{Lie algebra!free} on~$I$ exists.
\end{corollary}


\begin{proof}
  We have seen in \cref{uea of free lie algebra} that the universal enveloping algebra of the desired free~{\liealgebra{$\kf$}}~$F(I)$ is given by the free~{\algebra{$\kf$}}~$\kf\gen{x_i \suchthat i \in I}$.
  Since then we have learned that~$F(I)$ will therefore be a Lie~subalgebra of~$\kf\gen{x_i \suchthat i \in I}$.
  We thus define~$F(I)$ to be the Lie~subalgebra of~$\kf\gen{x_i \suchthat i \in I}$ generated by the variables~$x_i$ and let~$\iota \colon I \to \glie$ be the inclusion~$i \mapsto x_i$.
  We need to check that~$(F(I), \iota)$ defined in this way is indeed a free~{\liealgebra{$\kf$}} on~$I$.
  
  So let~$\hlie$ be any other~{\liealgebra{$\kf$}} and let~$\phi \colon I \to \hlie$ be any set-theoretic map.
  Then there exists by the universal property of the free~{\algebra{$\kf$}}~$\kf\gen{x_i \suchthat i \in I}$ a (unique) algebra homomorphism~$\Phi' \colon \kf\gen{x_i \suchthat i \in I} \to \Univ(\hlie)$ with~$\Phi'(x_i) = \phi(i)$ for every~$i \in I$.
  The image of~$F(I)$ under~$\Phi'$ lies in the Lie~subalgebra of~$\Univ(\hlie)$ that is generated by all~$\phi(i)$ with~$i \in I$, which in turn is contained in~$\hlie$.
  Hence the algebra homomorphism~$\Phi'$ restricts to a linear map~$\Phi \colon F(I) \to \hlie$, which is again a homomorphism of Lie~algebras.
  It follows from the commutativity of the square diagram
  \[
    \begin{tikzcd}[column sep = large]
      I
      \arrow{r}[above]{\phi}
      \arrow{d}[left]{i \mapsto x_i}
      &
      \hlie
      \arrow{d}
      \\
      \kf\gen{x_i \suchthat i \in I}
      \arrow[dashed]{r}[below]{\Phi'}
      &
      \Univ(F(I))
    \end{tikzcd}
  \]
  that the restricted diagram
  \[
    \begin{tikzcd}
      I
      \arrow{r}[above]{\phi}
      \arrow{d}[left]{\iota}
      &
      \hlie
      \arrow[equal]{d}
      \\
      F(I)
      \arrow[dashed]{r}[below]{\Phi}
      &
      \hlie
    \end{tikzcd}
  \]
  also commutes.
  We have thus shown the existence of an induced homomorphism~$\Phi \colon F(I) \to \hlie$ that makes the triangular diagram
  \[
    \begin{tikzcd}
      I
      \arrow{d}[left]{\iota}
      \arrow{r}[above]{\phi}
      &
      \hlie
      \\
      F(I)
      \arrow[dashed]{ur}[below right]{\Phi}
      &
      {}
    \end{tikzcd}
  \]
  commute.
  The uniqueness of this induced homomorphism follows from~$F(I)$ being generated by the image of~$\iota$.
\end{proof}


\begin{remark}
  Using the concept of free Lie~algebras one can define Lie~algebras by giving a set of generators~$X$ and a set of relations~$R \subseteq F(X)$.
\end{remark}


\begin{example}
  The Lie~algebra~$\sllie_2(\kf)$ can be defined by the generators~$X = \{e, h, f\}$ and the relations~$R = \{[h,e] - 2e, [h,f] + 2f, [e,f] - h\}$, which are usually written as~$[h,e] = 2e$,~$[h,f] = -2f$ and~$[e,f] = h$.
  
  One can more generally describe for every~$n \geq 1$ the Lie~algebra~$\sllie_{n+1}(\kf)$ by generators~$e_i$,~$f_i$, and~$h_i$ with~$i = 1, \dotsc, n$ subject to the relations
  \begin{align*}
    [h_i, h_j] = 0  \,,
    \quad
    [h_i, e_j] = a_{ij} e_j \,,
    \quad
    [h_i, f_j] = -a_{ij} f_j  \,,
    \quad
    [e_i, f_j] = \delta_{ij} h_i
  \end{align*}
  for all~$i,j = 1, \dotsc, n$ together with the relations
  \[
    \ad(e_i)^{1-a_{ij}}(e_j) = 0
    \quad\text{and}\quad
    \ad(f_i)^{1-a_{ij}}(f_j) = 0
  \]
  for all~$1 \leq i \neq j \leq n$, where the numbers~$a_{ij}$ are for all~$i,j = 1, \dotsc, n$ given by
  \[
    a_{ij} =
    \begin{cases}
    \phantom{-}2 & \text{if $i = j$}  \,, \\
              -1 & \text{if $|i-j| = 1$}  \,, \\
    \phantom{-}0 & \text{otherwise} \,.
    \end{cases}
  \]
\end{example}


\begin{corollary}
  \label{uea of direct sum of subspaces}
  Let~$\glie$ be a Lie algebra and let~$\hlie$ and~$\klie$ be two Lie~subalgebras of~$\glie$ with~$\glie = \hlie \oplus \klie$ as vector spaces.
  Then the multiplication map
  \[
    \Univ(\hlie) \tensor \Univ(\klie)
    \to
    \Univ(\glie) \,,
    \quad
    x \tensor y
    \mapsto
    xy
  \]
  is an isomorphism of vector spaces.
\end{corollary}


\begin{proof}
  Let~$(x_i)_{i \in I}$ be a basis of~$\hlie$ and let $(x_j)_{j \in J}$ be a basis of~$\klie$ where the index sets~$I$ and~$J$ are disjoint.
  Then $(x_k)_{k \in K}$ for the index set~$K \defined I \cup J$ is a basis of~$\glie$.
  Let~$(I, \leq)$ and~$(J, \leq)$ be linearly ordered.
  Then we can extend these two orders to a linear order on~$K$ such that~$i \leq j$ for all~$i \in I$ and~$j \in J$.
  We get from the PBW~theorem induced bases of~$\Univ(\hlie)$ and~$\Univ(\klie)$, and hence an induced basis of~$\Univ(\hlie) \tensor \Univ(\klie)$, as well as an induced basis of~$\Univ(\glie)$.
  The multiplication map~$\Univ(\hlie) \tensor \Univ(\klie) \to \Univ(\glie)$ is bijective on these bases and hence an isomorphism of vector spaces.
\end{proof}


\begin{remark}
  \Cref{uea of direct sum of subspaces} proves again that for all Lie~algebras~$\glie$ and~$\hlie$ the algebra homomorphism~$\Univ(\glie) \tensor \Univ(\hlie) \to \Univ(\glie \times \hlie)$ induced by the inclusions~$\glie \to \glie \times \hlie$ and~$\hlie \to \glie \times \hlie$ is an isomorphism of vector spaces, and hence an isomorphism of~{\algebras{$\kf$}}.
\end{remark}


\begin{remark}[The diamond lemma]
  The above proof of the PBW~theorem relies on a tactic from representation theory:
  Constructing an action on a suitable vector space for which the given generators act linearly independent (on some fixed element).
  We may ask if there is instead a proof which continues to study the commutator relation~$x_j x_i = x_i x_j + [x_i, x_j]$ with~$i, j \in I$ by exploiting that~$[x_i, x_j]$ is of smaller degree than~$x_j x_i$ and~$x_i x_j$.
  A similar question may also be asked for the Weyl~algebra.
  
  One way to do this is via the \defemph{diamond lemma}\index{diamond lemma}\index{lemma!diamond} as introduced by Bergman in~\cite{diamond_lemma}:
  Suppose that we are given an algebra~$A$ in the form of generators~$(x_i)_{i \in I}$ and relations~$\sigma \in \kf\gen{x_i \suchthat i \in I}$, $\sigma \in S$ (so that~$A = \kf\gen{x_i \suchthat i \in I}/(\sigma \suchthat \sigma \in S)$).
  Then the diamond lemma gives a criterion for showing that~$A$ admits a basis consisting of monomials in the~$x_i$, and also tells us how these monomials can be choosen.
  
  The main idea behind the diamond lemma is that one needs to be able to resolve ambiguities:
  We can represent every element of~$A$ as a linear combination of monomials in the~$x_i$, and hence as an element~$z \in \kf\gen{x_i \suchthat i \in I}$.
  Every relation~$\sigma$ between the generators can be seen as rewriting rules, that give us linear \enquote{rewriting maps}~$r_\sigma \colon \kf\gen{x_i \suchthat i \in I} \to \kf\gen{x_i \suchthat i \in I}$.
  To apply the diamond lemma we need two conditions:
  \begin{itemize}
    \item
      Whenever we apply rewritings~$r_{\sigma_1}, r_{\sigma_2}, \dotsc$ to an element~$z \in \kf\gen{x_i \suchthat i \in I}$ we want this process to stabilize after finitely many step.
    \item
      We need to be resolve ambiguities:
      Given an element~$z \in \kf\gen{x_i \suchthat i \in I}$ we can often apply different rewriting rules~$r_\sigma$ and~$r_\tau$ to it, resulting in different results~$r_\sigma(z)$ and~$r_\tau(z)$.
      We want to reconcile these different results by reducing both~$r_\sigma(z)$ and~$r_\tau(z)$ under finitely many rewritings to the same result~$z' \in \kf\gen{x_i \suchthat i \in I}$.
      We may visualize this process by the following diagram:
      \[
        \begin{tikzcd}[column sep = tiny]
          {}
          &
          z
          \arrow{dl}
          \arrow{dr}
          &
          {}
          \\
          r_\sigma(z)
          \arrow{dr}
          &
          {}
          &
          r_\tau(z)
          \arrow{dl}
          \\
          {}
          &
          z'
          &
          {}
        \end{tikzcd}
      \]
      The shape of this diagram is where the diamond lemma takes its name from.
  \end{itemize}
  
  Despite its usefulness the diamond is neither hard to state nor to prove.
  We strongly encourage the reader to check out the original paper \cite{diamond_lemma}.
  The main theorem and its proof in \cite[\S 1]{diamond_lemma} takes only three pages and is self-contained.
  In \cite[\S 3]{diamond_lemma} the PBW~theorem is then shown as an example.
\end{remark}




